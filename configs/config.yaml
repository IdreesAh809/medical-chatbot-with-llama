# try llama3:8b on cpu which is seem slow 
# model:
#   name: "llama3:8b"
#   temperature: 0.2
#   max_tokens: 150
#   concise: true

#now trying to use a quantized model
model:
  name: "nous-hermes:7b-llama2-q4_K_M"
  temperature: 0.2
  max_tokens: 150
  concise: true

embedding:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  dim: 384

chunk:
  size: 500
  overlap: 50

faiss:
  path: "data/embeddings/index.faiss"
